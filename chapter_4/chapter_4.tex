\documentclass[10pt]{article}

\usepackage{amssymb, amsfonts, amsmath, amsthm}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[margin=1.00 in]{geometry}
\usepackage{enumerate}
\usepackage{harpoon}
\usepackage{nicefrac}
\usepackage{tikz, pgfplots}
\usepackage{xypic}
\usepackage{tikz-cd}
\usepackage{booktabs}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\renewcommand{\headrulewidth}{0pt} % no line in header area
\fancyfoot{} % clear all footer fields
\fancyfoot[LE,RO]{\footnotesize \thepage}           % page number in "outer" position of footer line

\pgfplotsset{compat=newest}

%\setlength\parindent{0pt}%removes indents from entire file

\newtheorem*{sol}{Solution}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\Unif}{\operatorname{Uniform}}
\newcommand{\Bern}{\operatorname{Bernoulli}}
\newcommand{\Binom}{\operatorname{Binomial}}
\newcommand{\Poiss}{\operatorname{Poisson}}
\newcommand{\Gam}{\operatorname{Gamma}}
\newcommand{\se}{\operatorname{se}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\MSE}{\textsc{mse}}
\newcommand{\bias}{\mathsf{bias}}
\newcommand{\CDF}{\textsc{cdf}}
\newcommand{\MLE}{\textsc{mle}}

\begin{document}

\noindent \large{Solutions to selected exercises from Chapter 4 of
\emph{Wasserman --- All of Statistics}}

\begin{enumerate}
\item[(2)]
We have $\E(X)=\lambda=\V(X)$. Thus
\[
\P(X\geq 2\lambda) =\P(X-\lambda\geq \lambda)\leq \P(|X-\lambda|\geq \lambda)
\leq \lambda / \lambda^2=1/\lambda.
\]

\item[(3)]
We have $\E(\overline X_n)=p$ and (assuming the $X_i$ are independent)
\[
\V(\overline X_n) = \frac{1}{n^2} \sum \V(X_i) = \frac{1}{n^2} \sum p(1-p) =
\frac{p(1-p)}{n}.
\]

Chebyshev's inequality gives
\[
\P(|\overline X_n - p| > \epsilon) \leq \frac{p(1-p)}{n\epsilon^2}.
\]

Hoeffding's inequality gives
\[
\P(|\overline X_n - p| > \epsilon) \leq 2e^{-2n\epsilon^2}.
\]

The bound from Hoeffding's inequality divided by the bound from Chebyshev's
inequality approaches 0 as $\epsilon\to \infty$, as can be shown easily using
e.g. L'H\^opital's rule.

\item[(4)]
See the Jupyter Notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_4/4.ipynb}{4.ipynb}.

\item[(6)]
See the Jupyter Notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_4/6.ipynb}{6.ipynb}.

\item[(7)]
We have $\overline X_n \sim N(0,1/n)$ and hence
$\sqrt n \overline X_n \sim N(0,1)$. By Mill's inequality,
\[
\P(|\overline X_n| > t) = \P(\sqrt n |\overline X_n| > \sqrt n t)
\leq \sqrt{\frac{2}{\pi}} \frac{e^{-nt^2/2}}{\sqrt n t}.
\]
By Chebyshev's inequality,
\[
\P(|\overline X_n| \geq t) \leq \frac{1}{nt^2}.
\]
For $t\gg 0$, Mill's inequality gives a much better (i.e. smaller) upper bound
than Chebyshev's inequality. On ther other hand for $t\approx 0$ neither
inequality tells us anything useful.

\end{enumerate}

\end{document}
