\documentclass[10pt]{article}

\usepackage{amssymb, amsfonts, amsmath, amsthm}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[margin=1.00 in]{geometry}
\usepackage{enumerate}
\usepackage{harpoon}
\usepackage{nicefrac}
\usepackage{tikz, pgfplots}
\usepackage{xypic}
\usepackage{tikz-cd}
\usepackage{booktabs}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\renewcommand{\headrulewidth}{0pt} % no line in header area
\fancyfoot{} % clear all footer fields
\fancyfoot[LE,RO]{\footnotesize \thepage}           % page number in "outer" position of footer line

\pgfplotsset{compat=newest}

%\setlength\parindent{0pt}%removes indents from entire file

\newtheorem*{sol}{Solution}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\Unif}{\operatorname{Uniform}}
\newcommand{\Bern}{\operatorname{Bernoulli}}
\newcommand{\Binom}{\operatorname{Binomial}}
\newcommand{\Poiss}{\operatorname{Poisson}}
\newcommand{\se}{\operatorname{se}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Cov}{\operatorname{Cov}}

\begin{document}

\noindent \large{Solutions to selected exercises from Chapter 3 of
\emph{Wasserman --- All of Statistics}}

\begin{enumerate}
\item[(1)]
Suppose you halve your money $m$ times out of $n$. There are $\binom{n}{m}$
ways to do this, each with probability $(1/2)^n$. Your fortune after $n$
trials is $(1/2)^m2^nc$. We get the expected value of your fortune after $n$
trials by summing over $m$:
\[
\sum_{m=0}^n \binom{n}{m}\left(\frac{1}{2}\right)^n \left(\frac{1}{2}\right)^m2^{n-m}c
= \sum_{m=0}^n \binom{n}{m} \left(\frac{1}{2}\right)^{2m}c.
\]
We can recognize this as
\[
\sum_{m=0}^n \binom{n}{m} \left(\frac{1}{4}\right)^m(1)^{n-m} =
c\left(1+\frac{1}{4}\right)^n = c\left(\frac{5}{4}\right)^n.
\]

\item[(3)]
We suppose the $X_i$ are independent. For $x\in [0,1]$, the CDF of $Y$ satisfies
\[
F_Y(x)=\P(X_i\leq x \text{ for all } i)=x^n
\]
and $f_Y(x) = nx^{n-1}$ on $[0,1]$ and $0$ elsewhere.

Thus
\[
\E(Y) = \int_0^1 x (n x^{n-1}) dx =\frac{n}{n+1}.
\]

\item[(4)]
Let $Y_1,\ldots,Y_n$ be independent $\Bern(p)$ random variables with $Y_i=0$
if the particle moves to the right on the $i^{\text{th}}$ step and $Y_i=1$
otherwise. Then
\[
X_n = \sum_{i=1}^n (1-2Y_i).
\]
We have $\E(X_n) = \sum_{i=0}^n \E(1-2Y_i)$ and
$\E(1-2Y_i) = (1-p)(1) + p(-1)=1-2p$ so $\E(X_n)=n(1-2p)$.

By independence, the variance is $\V(X_n) = \sum_{i=0}^n \V(1-2Y_i)$.
We have $(1-2Y_i)^2=1$ always so $\E(((1-2Y_i)^2)=1$. So
\[
\V(1-2Y_i) = \E((1-2Y_i)^2) - \E(1-2Y_i)^2 = 1 - (1-2p)^2 = 4p-4p^2
\text{ and } \V(X_n) = 4np(1-p).
\]

\item[(5)]
The sample space is $\{H, TH, TTH, TTTH,\ldots\}$ with the $i^{th}$ entry
occurring with probability $1/2^i$. The expected value is thus
\[
\frac{1}{2}(1) + \frac{1}{4}(2) + \frac{1}{8}(3) + \ldots =
\sum_{i=0}^\infty \frac{i}{2^i}.
\]
Recall that the Taylor series of $1/(1-x^2)$ is $\sum i x^{i-1}$. So
the expected value is
\[
\frac{1}{2}\sum_{i=0}^\infty \frac{i}{2^{i-1}} =
\frac{1}{2} \frac{1}{(1-\frac{1}{2})^2} = 2.
\]

\item[(9)]
See the Jupyter notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_3/9.ipynb}{9.ipynb}.

\item[(10)]
By the rule of the lazy statistician,
\[
\E(Y) = \frac{1}{\sqrt{2\pi}} \int e^x e^{-x^2/2}dx
= \frac{1}{\sqrt{2\pi}} \int e^{-(x-1)^2/2 + 1/2}dx
= \sqrt{e} \int \frac{1}{\sqrt{2\pi}} e^{-(x-1)^2/2}dx.
\]
The integral $\int \frac{1}{\sqrt{2\pi}} e^{-(x-1)^2/2}dx$ is just the total
integral of the PDF of a $N(1,1)$ random variable so it's equal to 1. Finally
then $\E(Y)=\sqrt{e}$.

To compute the variance we consider $Y^2 = e^{2X}$. We have
\[
\E(Y^2) = \int \frac{1}{\sqrt{2\pi}} e^{1/2(4x-x^2)}dx =
\int \frac{1}{\sqrt{2\pi}} e^{-1/2(2-x)^2+2}dx =
e^2 \int \frac{1}{\sqrt{2\pi}} e^{-1/2(2-x)^2}dx = e^2.
\]

Finally then
\[
\V(Y) = \E(Y^2) - \E(Y)^2 = e^2-e.
\]

\item[(11)]
\begin{enumerate}[(a)]
\item
First
\[
\E(X_n) = \sum \E(X_i) = 0.
\]
We have $\V(X_i)=\frac{1}{2}(1-0)^2 + \frac{1}{2}(-1-0)^2=1$ so
\[
\V(X_n) = \sum \V(X_i) = n.
\]
\item See the Jupyter notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_3/11.ipynb}{11.ipynb}.
\end{enumerate}

\item[(15)]
Set $Z=r(X,Y)=2X-3Y$. We have $\V(2X-3Y+8)=\V(Z)=\E(Z^2) - \E(Z)^2$.
We'll calculate these two terms separately.
\[
\E(Z) = \int_0^2 \int_0^1(2x-3y) \frac{1}{3}(x+y)dxdy=-\frac{23}{9}.
\]
And $Z^2=4X^2-12XY+9Y^2$ so
\[
\E(Z^2)=\int_0^2\int_0^1 (4x^2-12xy+9y^2)\frac{1}{3}(x+y)dxdy=\frac{86}{9}.
\]
So
\[
\V(Z)= \frac{86}{9} - \left(\frac{23}{9}\right)^2 = \frac{245}{81}.
\]

\item[(21)]
By hypothesis,
\[
x = \E(Y | X=x) = \int y \frac{f_{X,Y}(x,y)}{f_X(x)}dy
\]
i.e. $\int y f_{X,Y}(x,y)dy = x f_X(x)$.

Letting $\mu_X,\mu_Y$ be the expectations, we have
\[
\Cov(X,Y) = \E((X-\mu_X)(Y-\mu_Y)) =
\int \int (x-\mu_X)(y-\mu_Y)f_{X,Y}(x,y)dxdy
\]
and this is equal to
\[
\int (x-\mu_X)\left(\int (y-\mu_Y) f_{X,Y}(x,y)dy\right)dx
= \int (x-\mu_X) \left(xf_X(x) - \mu_Yf_X(x)\right)dx
\]
using that $\int y f_{X,Y}(x,y)dy = x f_X(x)$.

Now note by the Rule of Iterated Expectations that
\[
\mu_Y = \E(Y) = \E(\E(Y|X)) = \E(X) = \mu_X.
\]
plugging this into the equality from the last paragraph gives
\[
\Cov(X,Y) = \int (x-\mu_X)(x-\mu_Y)f_X(x)dx = \int(x-\mu_X)^2 f_X(x)dx=\V(X).
\]

\item[(22)]
\begin{enumerate}[(a)]
\item $Y$ and $Z$ are not independent. For $\P(Y=1,Z=1)=b-a$ whereas
$\P(Y=1)\P(Z=1) = b(1-a) > b-a$.
\item $\E(Y|Z=z) = \sum y f_{Y|Z}(y|z)$ and $f_{Y|Z}(y|z) = f_{Y,Z}(y,z)/f_Z(z)$.
The distribution $f_{Y,Z}$ is given by the following table:
\begin{center}
\begin{tabular}{c | c | c}
$y/z$ & $0$ & $1$ \\
\hline
$0$ & $0$ & $1-b$ \\
\hline
$1$ & $a$ & $b-a$
\end{tabular}
\end{center}

We have $f_Z(0)=a$ and $f_Z(1)=1-a$. So $f_{Y|Z}$ is given by the following table:
\begin{center}
\begin{tabular}{c | c | c}
$y/z$ & 0 & 1 \\
\hline
$0$ & $0$ & $\frac{1-b}{1-a}$ \\
\hline
$1$ & $1$ & $\frac{b-a}{1-a}$
\end{tabular}
\end{center}

Thus,
\[
\E(Y|Z=0) = 0 \cdot f_{Y|Z}(0,0) + 1 \cdot f_{Y|Z}(1,0) = f_{Y|Z}(1,0) = 1
\]
and
\[
\E(Y|Z=1) = f_{Y|Z}(1,1) = \frac{b-a}{1-a}
\]
so that
\[
\E(Y|Z) =
\begin{cases}
1 & Z = 0 \\
\frac{b-a}{1-a} & Z =1.
\end{cases}
\]
\end{enumerate}

\item[(23)]
\underline{$\Poiss(\lambda)$}
\[
\psi_X(t) = \sum_{x=0}^\infty e^{tx} \frac{e^{-\lambda}\lambda^x}{x!} =
e^{-\lambda} \sum_{x=0}^\infty \frac{e^{(t+\log\lambda)x}}{x!} =
e^{-\lambda}e^{e^{t+\log\lambda}}=e^{\lambda(e^t-1)}
\]

\underline{$N(\mu,\sigma)$}
\[
\psi_X(t) =
\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} e^{tx}e^{-(x-\mu)^2/2\sigma^2}dx =
\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} e^{(-x^2+2\sigma^2tx + 2\mu x -\mu^2)/2\sigma^2}dx.
\]
Completing the square yields
\[
\psi_X(t) = e^{t\mu + \sigma^2t^2/2} \int \frac{1}{\sqrt{2\pi}\sigma}
e^{-(x-\sigma^2t-\mu)^2/2\sigma^2}dx = e^{t\mu + \sigma^2t^2/2}.
\]

\underline{$\Gamma(\alpha)$}
\[
\psi_X(t) =
\int_0^\infty e^{tx} \frac{1}{\beta^\alpha \Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta}dx =
\frac{1}{(1-t\beta)^\alpha}
\int_0^\infty \frac{1}{\left(\frac{\beta}{1-t\beta}\right)^\alpha \Gamma(\alpha)}
e^{-x(1-t\beta)/\beta}dx
\]
and this is just equal to $\frac{1}{(1-t\beta)^\alpha}$
(assuming that $t<1/\beta$).
\end{enumerate}
\end{document}
