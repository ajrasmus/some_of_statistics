\documentclass[10pt]{article}

\usepackage{amssymb, amsfonts, amsmath, amsthm}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[margin=1.00 in]{geometry}
\usepackage{enumerate}
\usepackage{harpoon}
\usepackage{nicefrac}
\usepackage{tikz, pgfplots}
\usepackage{xypic}
\usepackage{tikz-cd}
\usepackage{booktabs}
\usepackage{systeme}


\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\renewcommand{\headrulewidth}{0pt} % no line in header area
\fancyfoot{} % clear all footer fields
\fancyfoot[LE,RO]{\footnotesize \thepage}           % page number in "outer" position of footer line

\pgfplotsset{compat=newest}

%\setlength\parindent{0pt}%removes indents from entire file

\newtheorem*{sol}{Solution}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Unif}{\operatorname{Uniform}}
\newcommand{\Bern}{\operatorname{Bernoulli}}
\newcommand{\Binom}{\operatorname{Binomial}}
\newcommand{\Poiss}{\operatorname{Poisson}}
\newcommand{\se}{\operatorname{se}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\ol}{\overline}

\begin{document}

\noindent \large{Solutions to selected exercises from Chapter 11 of
\emph{Wasserman --- All of Statistics}}

\begin{enumerate}[(1)]
\item[(1)]
We have $f(\theta|X^n)\propto f(\theta)\mathcal L(\theta)$. Thus,
\[
    f(\theta|X^n)\propto
    e^{-(\theta-a)^2/2b^2} \prod_i e^{-(X_i-\theta)^2/2\sigma^2} =
    \exp\left(-(\theta-a)^2/2b^2 - \sum (\theta - X_i)^2/2\sigma^2\right).
\]
Consider $\frac{(\theta-a)^2}{2b^2} + \sum \frac{(\theta - X_i)^2}{2\sigma^2}$.
Up to adding a constant independent of $\theta$, this is equal to
\[
    \frac{(\theta-a)^2}{2b^2} + \frac{n(\theta - \bar X)^2}{2\sigma^2},
\]
since $\sum (\theta - X_i)^2=\theta^2 - 2n\bar X \theta + \sum X_i^2$.
This is in turn equal to
\[
    \frac{\sigma^2(\theta - a)^2 + nb^2(\theta - \bar X)^2}{2b^2\sigma^2}.
\]
Expanding this out yields
\[
    \frac{(\sigma^2 + nb^2)\theta^2 - 2(\sigma^2a + b^2 n\bar X)\theta + C}
    {2b^2\sigma^2}
\]
where $C$ is some constant independent of $\theta$. Up to adding another constant
not depending on $\theta$ (by completing the square), this is equal to
\[
    \frac{(\sigma^2 + nb^2)\left(\theta - \frac{\sigma^2a + b^2 n\bar X}
    {\sigma^2+nb^2}\right)^2}
    {2b^2\sigma^2}.
\]
Thus, up to multiplying by a constant not depending on $\theta$, $f(\theta|X^n)$
is equal to
\[
    \exp\left(-\frac{\sigma^2+nb^2}{2b^2\sigma^2}
    \left(\theta-\frac{\sigma^2a+b^2n\bar X}{\sigma^2+nb^2}\right)^2\right).
\]
Finally, compute that $\frac{1}{\tau^2}=\frac{\sigma^2+nb^2}{b^2\sigma^2}$
and $\bar \theta = w\bar X + (1-w)a=\frac{\sigma^2a+b^2n\bar X}{\sigma^2 + nb^2}$
to conclude that $\theta|X^n \sim N(\bar \theta, \tau^2)$, as claimed.


\item[(2)]
See the Jupyter Notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_11/2.ipynb}{2.ipynb}.


\item[(3)] Set $M=\max\{x_1,\ldots,x_n\}$. Then
\[
    \mathcal L(\theta) = \begin{cases}
        0 & \text{if } \theta < M \\
        \theta^{-n} & \text{if } \theta \geq M \\
    \end{cases}.
\]
Thus,
\[
    f(\theta | x^n) \propto f(\theta)\mathcal L(\theta) =
    \begin{cases}
        0 & \text{if } \theta < M \\
        \theta^{-n-1} & \text{if } \theta \geq M
    \end{cases}.
\]
The integral is $\int_M^\infty \theta^{-n-1} = \frac{1}{n}M^{-n}$.
Hence
\[
    f(\theta|x^n) = \begin{cases}
        \frac{n\max\{x_1,\ldots,x_n\}^n}{\theta^{n+1}} &
        \text{if } \theta \geq \max\{x_1,\ldots,x_n\} \\
        0 & \text{else }
    \end{cases}.
\]

\item[(4)]
See the Jupyter Notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_11/4.ipynb}{4.ipynb}.

\item[(5)]
See the Jupyter Notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_11/5.ipynb}{5.ipynb}.

\item[(6)]
\begin{enumerate}
    \item We have
        \[
            f(\lambda|x^n) \propto f(\lambda)\mathcal L_n(\lambda) \propto
            \lambda^{\alpha-1}e^{-\lambda/\beta} \prod_{i=1}^n e^{-\lambda}
            \frac{\lambda^{x_i}}{x_i!}
            \propto \lambda^{\alpha+\sum x_i-1}e^{-\lambda/\beta-n\lambda}.
        \]
        Thus,

        \[
            \lambda|x^n \sim
            \Gamma\left(\alpha + \sum x_i,\frac{1}{n+\frac{1}{\beta}}\right).
        \]
        The posterior mean is the product of the two parameters:
        \[
            \bar \lambda = \frac{\alpha +\sum x_i}{n+\frac{1}{\beta}}.
        \]
    \item We have
    \[
        \log(f(x;\lambda)) = -\lambda + x\log \lambda - \log(x!).
    \]
    Therefore
    \[
        \frac{\partial}{\partial \lambda} \log(f(x;\lambda)) =
        -1 +\frac{x}{\lambda}
        \text{ and }
        \frac{\partial^2}{\partial \lambda^2} \log(f(x;\lambda)) =
        -\frac{x}{\lambda^2}.
    \]
    By Theorem 9.17, we have
    \[
        I(\theta) =
        -E\left(-\frac{X}{\lambda^2}\right)
        =\sum_{x=0}^\infty e^{-\lambda} \frac{\lambda^x x}{x!\lambda^2}
        =\frac{1}{\lambda}\sum_{x=1}^\infty e^{-\lambda}\frac{\lambda^{x-1}}{(x-1)!}
        =\frac{1}{\lambda}.
    \]
    Therefore the Jeffreys' prior is $\frac{1}{\sqrt{\lambda}}$. This is an
    improper prior since the integral of $\lambda^{-1/2}$ is infinite.

    The posterior with respect to the Jeffreys' prior is
    \[
        f(\lambda|x^n)
        \propto \lambda^{-1/2} \prod_{i=1}^n
        e^{-\lambda} \frac{\lambda^{x_i}}{x_i!}
        \propto \lambda^{-1/2}e^{-n\lambda} \lambda^{\sum x_i}
        =\lambda^{\sum x_i +1/2-1}e^{-n\lambda}.
    \]
    Therefore
    \[
        \lambda|x^n \sim \Gamma\left(\sum x_i + \frac{1}{2}, \frac{1}{n}\right).
    \]

\end{enumerate}

\item[(8)]
See the Jupyter Notebook
\href{https://github.com/ajrasmus/some_of_statistics/blob/main/chapter_11/8.ipynb}{8.ipynb}.

\end{enumerate}
\end{document}
