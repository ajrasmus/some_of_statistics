{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79de06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from numpy.linalg import inv, det\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from functools import partial\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7810e64",
   "metadata": {},
   "source": [
    "First let's prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3b88b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spam.txt', 'r') as f:\n",
    "    arr = []\n",
    "    for line in f:\n",
    "        row = line.split()\n",
    "        row = list(map(lambda x: float(x), row))\n",
    "        arr.append(row)\n",
    "\n",
    "df = pd.DataFrame(arr)\n",
    "df[57] = df[57].astype(int)\n",
    "\n",
    "ds = df.to_numpy()\n",
    "\n",
    "X = ds[:,:-1]\n",
    "Y = ds[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5304440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9689faf",
   "metadata": {},
   "source": [
    "We change the class labels from 0 and 1 to -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "640abe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[Y == 0.] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4516c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_with_weights(Y, weights, indices):\n",
    "    \"\"\"\n",
    "        Calculate the proportion of classes in Y in each\n",
    "        class where the indices lie in indices.\n",
    "        The proportion will actually be weighted by the array\n",
    "        weights so that indices with higher weights will\n",
    "        affect the proportion more.\n",
    "        Y: a numpy array of class labels\n",
    "        weights: a 1-d numpy array of weights with total sum 1\n",
    "            and Y.shape = weights.shape\n",
    "        indices: the indices to compute the proportions over\n",
    "        returns:\n",
    "            p0, p1: the weighted proportions\n",
    "    \"\"\"\n",
    "    p0 = np.dot(Y[indices] == -1, weights[indices]) / len(Y[indices])\n",
    "    p1 = np.dot(Y[indices] == 1, weights[indices]) / len(Y[indices])\n",
    "    \n",
    "    return p0, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fd5950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_indices(X, Y, indices, col, t):\n",
    "    \"\"\"\n",
    "        Calculate new indices obtained by splitting on the column col\n",
    "        according to col <= t or col > t. The new indices are obtained\n",
    "        by restricting (X,Y) to the rows specified by indices.\n",
    "        X, Y: arrays with X.shape[0] = Y.shape[0]\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        col: the column specifying a feature to split on\n",
    "        t: the value defining the split according to col <= or col > t\n",
    "        Returns:\n",
    "            new_indices: a pair of subsets of indices which\n",
    "            partition indices when taken together\n",
    "    \"\"\"\n",
    "    new_indices = (np.intersect1d(indices, np.where(X[:, col] <= t)),\n",
    "            np.intersect1d(indices, np.where(X[:, col] > t)))\n",
    "    return new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7d1751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_impurity_with_weights(X, Y, weights, indices, col, t):\n",
    "    \"\"\"\n",
    "        Calculate the Gini impurity (with weights) after splitting (X,Y)\n",
    "        on the column col according to col <= t or col > t.\n",
    "        X, Y: arrays with X.shape[0] = Y.shape[0]\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        weights: an array with weights.shape = Y.shape\n",
    "        col: the column specifying a feature to split on\n",
    "        t: the value defining the split according to col <= or col > t\n",
    "    \"\"\"\n",
    "    \n",
    "    new_indices = split_to_indices(X, Y, indices, col, t)\n",
    "    \n",
    "    gammas = []\n",
    "    \n",
    "    for new_ind in new_indices:\n",
    "        p0, p1 = proportion_with_weights(Y, weights, new_ind)\n",
    "        gamma = 1 - p0**2 - p1**2\n",
    "        gammas.append(gamma)\n",
    "    \n",
    "    impurity = np.array(gammas).sum()\n",
    "    \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da1e9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_with_weights(X, Y, indices, weights, n_0=500, frac=4, num_ts=20):\n",
    "    \"\"\"\n",
    "        Calculate a split to minimize the gini impurity.\n",
    "        X, Y: arrays with X.shape[0] = Y.shape[0]\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        weights: an array with weights.shape = Y.shape\n",
    "        n_0: do not compute a split if there are fewer than this\n",
    "            number of indices in indices\n",
    "        frac: only create a split in which both subsets of the\n",
    "            partition of indices contain at least n_0 / frac elements\n",
    "        num_ts: the number of values of t to test as in col <= t\n",
    "            and col < t\n",
    "        Returns:\n",
    "            best_split: a partition of indices according to the best\n",
    "            split\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(indices) <= n_0:\n",
    "        return (None, None)\n",
    "    else:\n",
    "        best_split = (None, None)\n",
    "        min_impurity = float('inf')\n",
    "\n",
    "        k = X.shape[1]\n",
    "        for col in range(k):\n",
    "            low = X[indices, col].min()\n",
    "            high = X[indices, col].max()\n",
    "            ts = np.linspace(low, high, num_ts)\n",
    "\n",
    "            for t in ts:\n",
    "                impurity = calc_impurity_with_weights(X, Y, weights, indices, col, t)\n",
    "                new_indices = split_to_indices(X, Y, indices, col, t)\n",
    "                if len(new_indices[0]) >= n_0 / frac and len(new_indices[1]) >= n_0 / frac:\n",
    "                    if impurity < min_impurity:\n",
    "                        min_impurity = impurity\n",
    "                        best_split = (col, t)\n",
    "\n",
    "        return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9e658e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "        Represents a node of a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y, indices, col=None, t=None, lchild=None, rchild=None):\n",
    "        \"\"\"\n",
    "            (X, Y): the dataset to train on\n",
    "            indices: the indices corresponding to the split\n",
    "                represented by this node\n",
    "            col: a column to split on, if the node has children\n",
    "            t: the value for the split according to col <= t or col > t\n",
    "            lchild, rchild: nodes representing the left and right child\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.avg = Y[indices].mean()\n",
    "        self.indices = indices\n",
    "        self.col = col\n",
    "        self.t = t\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "    \n",
    "    def split(self, weights, n_0, frac, num_ts):\n",
    "        \"\"\"\n",
    "            Split the node using generate_split_with_weights and assign\n",
    "            its children, column, and t\n",
    "        \"\"\"\n",
    "        (col, t) = generate_split_with_weights(self.X, self.Y, self.indices, weights, n_0, frac, num_ts)\n",
    "        if col is not None:\n",
    "            new_indices = split_to_indices(self.X, self.Y, self.indices, col, t)\n",
    "            self.col = col\n",
    "            self.t = t\n",
    "            self.lchild = Node(self.X, self.Y, new_indices[0])\n",
    "            self.rchild = Node(self.X, self.Y, new_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cec96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "        Represents a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y, weights=np.ones(len(Y)) / len(Y), n_0=500, frac=4, num_ts=20):\n",
    "        \"\"\"\n",
    "            Initialize the tree with a single node.\n",
    "            (X, Y): the dataset to train on\n",
    "            weights: an array with weights.shape=Y.shape representing\n",
    "                weights for the different examples of the dataset\n",
    "            n_0: an integer such that nodes of size <= n_0 are not split\n",
    "                further\n",
    "            frac: an integer such that each node has size >= n_0/frac\n",
    "            num_ts: the number of values of t to test for each column\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.weights = weights      \n",
    "        self.n_0 = n_0\n",
    "        self.frac = frac\n",
    "        self.num_ts = num_ts\n",
    "        \n",
    "        # Initialize the tree with a single node\n",
    "        # representing all indices of the dataset.\n",
    "        # The children of node i in self.tree\n",
    "        # lie at indices 2*i+1 and 2*i+2.\n",
    "        self.root = Node(X, Y, list(range(len(X))))\n",
    "        self.tree = [self.root]\n",
    "        \n",
    "        self.n_levels = 1 \n",
    "    \n",
    "    def split_leaves(self):\n",
    "        \"\"\"\n",
    "            Split each leaf node.\n",
    "            Returns:\n",
    "                add_new_nodes: a Boolean saying whether any new\n",
    "                nodes were created by splitting or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # The leaves of the tree come at the end\n",
    "        leaves = self.tree[-2**(self.n_levels-1):]\n",
    "        \n",
    "        # An array to hold the new children of the leaves\n",
    "        children = []\n",
    "        \n",
    "        for node in leaves:\n",
    "            \n",
    "            # Add the children created by the split for each leaf node.\n",
    "            # If the node is None then its children will be None.\n",
    "            if node is not None:\n",
    "                node.split(self.weights, self.n_0, self.frac, self.num_ts)\n",
    "                children += [node.lchild, node.rchild]\n",
    "            else:\n",
    "                children += [None, None]\n",
    "        \n",
    "        # A boolean recording whether any new children have been created\n",
    "        add_new_nodes = any(children)\n",
    "        \n",
    "        # If there are new children, add them to the tree.\n",
    "        if add_new_nodes:\n",
    "            self.tree += children\n",
    "            self.n_levels += 1\n",
    "        \n",
    "        return add_new_nodes\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            Split all leaf nodes until none can be split further\n",
    "            according to our end condition.\n",
    "        \"\"\"\n",
    "        continue_splitting = True\n",
    "        \n",
    "        while continue_splitting:\n",
    "            continue_splitting = self.split_leaves()\n",
    "    \n",
    "    def predict_flt_one(self, x):\n",
    "        \"\"\"\n",
    "            Predict a float for the array x of shape\n",
    "            X.shape[1]. This float will be the average\n",
    "            of the classes for the node in which x lies\n",
    "            in the decision tree.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        \n",
    "        \n",
    "        # Traverse the decision tree until coming to the\n",
    "        # node which contains x.\n",
    "        while node.col:\n",
    "            if x[node.col] > node.t:\n",
    "                node = node.rchild\n",
    "            else:\n",
    "                node = node.lchild\n",
    "        \n",
    "        return node.avg\n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        \"\"\"\n",
    "            Predict a class for the array x of\n",
    "            shape X.shape[1].\n",
    "        \"\"\"\n",
    "        return np.sign(self.predict_flt_one(x))\n",
    "    \n",
    "    def predict_flt(self, x):\n",
    "        \"\"\"\n",
    "            Predict floats for the array x with\n",
    "            x.shape[1] = X.shape[1].\n",
    "        \"\"\"\n",
    "        return np.array(list(map(self.predict_flt_one, x)))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "            Predict classes for the array x with\n",
    "            x.shape[1] = X.shape[1].\n",
    "        \"\"\"\n",
    "        return np.array(list(map(self.predict_one, x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cccf517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_step(X, Y, weights, n_0=500, frac=4, num_ts=20):\n",
    "    \"\"\"\n",
    "        Perform a single step of adaboost. Train a decision\n",
    "        tree with the given weights. Return the prediction function\n",
    "        for this decision tree, new weights, and an alpha value.\n",
    "        X, Y: the dataset\n",
    "        weights: the weights used for the decision tree\n",
    "        n_0: the minimum number of data points in a node to split\n",
    "        frac: the number such that each node has at least n_0 / frac data points\n",
    "        num_ts: the number of values of t to test for each column\n",
    "        returns:\n",
    "            clf: the classifier for the trained tree\n",
    "            alpha: a weighting for the tree\n",
    "            new_weights: the new weights to use in the next\n",
    "                adaboost step\n",
    "    \"\"\"\n",
    "    tree = DecisionTree(X, Y, weights, n_0, frac, num_ts)\n",
    "    tree.train()\n",
    "    clf = tree.predict\n",
    "    Y_pred = clf(X)\n",
    "    err = np.dot(weights, Y != Y_pred) / weights.sum()\n",
    "    alpha = np.log((1 - err) / err)\n",
    "    \n",
    "    indicator = Y != Y_pred\n",
    "    new_weights = weights * np.exp(alpha * indicator)\n",
    "    \n",
    "    return clf, alpha, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80e2388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(X, Y, n_steps=20, n_0=500, frac=4, num_ts=20):\n",
    "    \"\"\"\n",
    "        Perform adaboost for a specified number of steps.\n",
    "        X, Y: the dataset\n",
    "        n_0: the minimum number of data points in a node to split\n",
    "        frac: the number such that each node has at least n_0 / frac data points\n",
    "        num_ts: the number of values of t to test for each column\n",
    "        returns:\n",
    "            clfs_coeffs: an array of (clf, alpha) pairs\n",
    "                where clf is the prediction function for a single\n",
    "                decision tree and alpha is the weight to use\n",
    "                for that tree in the adaboost model\n",
    "    \"\"\"\n",
    "    \n",
    "    # The initial weights are uniform\n",
    "    init_weights = np.ones(len(Y)) / len(Y)\n",
    "    weights = init_weights\n",
    "    clfs_coeffs = []\n",
    "    \n",
    "    for j in range(n_steps):\n",
    "        # Perform a single step of adaboost and print\n",
    "        # the relevant data from training\n",
    "        print(f'Step {j+1} / {n_steps}')\n",
    "        clf, alpha, new_weights = adaboost_step(X, Y, weights, n_0=n_0, frac=frac, num_ts=num_ts)\n",
    "        print(f'alpha: {alpha}')\n",
    "        print(f'weights: {new_weights}')\n",
    "        clfs_coeffs.append((clf, alpha))\n",
    "        weights = new_weights\n",
    "    \n",
    "    return clfs_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc9946e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_of_combo_clf(clfs_coeffs):\n",
    "    \"\"\"\n",
    "        Given an array of (clf, alpha) pairs, return a function\n",
    "        which gives the sign of the sum of weighted terms alpha * clf(x)\n",
    "        for any input x.\n",
    "    \"\"\"\n",
    "    return lambda x: np.sign(np.sum([alpha * clf(x) for (clf, alpha) in clfs_coeffs], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fda029",
   "metadata": {},
   "source": [
    "Now we'll perform cross-validation to compare Adaboost and a base tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4ccc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_splits(X, Y, num_ch):\n",
    "    \"\"\"\n",
    "        Split the dataset (X, Y) into num_ch random chunks\n",
    "        of equal size (except for possibly the last chunk).\n",
    "        (X, Y): the dataset\n",
    "        num_ch: the number of chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly permute (X, Y)\n",
    "    n = len(X)\n",
    "    indices = np.random.permutation(n)\n",
    "    X_perm = X[indices]\n",
    "    Y_perm = Y[indices]\n",
    "    \n",
    "    k = n // num_ch\n",
    "    \n",
    "    # Arrays to hold the chunks\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    \n",
    "    # Append one chunk at a time\n",
    "    for i in range(num_ch):\n",
    "        if i < num_ch-1:\n",
    "            Xs.append(X_perm[i*k:i*k + k, :])\n",
    "            Ys.append(Y_perm[i*k:i*k + k])\n",
    "        else:\n",
    "            Xs.append(X_perm[i*k:, :])\n",
    "            Ys.append(Y_perm[i*k:])\n",
    "    \n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ab012fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/7qb3ygvs0mzb0cdjbh8z8svr0000gn/T/ipykernel_57155/1681302256.py:15: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p0 = np.dot(Y[indices] == -1, weights[indices]) / len(Y[indices])\n",
      "/var/folders/h9/7qb3ygvs0mzb0cdjbh8z8svr0000gn/T/ipykernel_57155/1681302256.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p1 = np.dot(Y[indices] == 1, weights[indices]) / len(Y[indices])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 20\n",
      "alpha: 2.3970065155187164\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00027167]\n",
      "Step 2 / 20\n",
      "alpha: 0.7953915819247714\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00060182]\n",
      "Step 3 / 20\n",
      "alpha: 0.43284005004398907\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 4 / 20\n",
      "alpha: 0.036245684567853445\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 5 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 6 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 7 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 8 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 9 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 10 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 11 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 12 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 13 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 14 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 15 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 16 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 17 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 18 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 19 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 20 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00092779]\n",
      "Step 1 / 20\n",
      "alpha: 2.3212547108366017\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00027167]\n",
      "Step 2 / 20\n",
      "alpha: 0.7743826889320945\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00058931]\n",
      "Step 3 / 20\n",
      "alpha: 0.3884910567768446\n",
      "weights: [0.00027167 0.00040064 0.00027167 ... 0.00027167 0.00027167 0.00086909]\n",
      "Step 4 / 20\n",
      "alpha: -0.041345744652063185\n",
      "weights: [0.00027167 0.00038441 0.00027167 ... 0.00027167 0.00027167 0.00083389]\n",
      "Step 5 / 20\n",
      "alpha: 0.11685846321541347\n",
      "weights: [0.00027167 0.00043206 0.00027167 ... 0.00027167 0.00027167 0.00093726]\n",
      "Step 6 / 20\n",
      "alpha: -0.07429384172900788\n",
      "weights: [0.00027167 0.00040113 0.00027167 ... 0.00025221 0.00027167 0.00087015]\n",
      "Step 7 / 20\n",
      "alpha: 0.05081832110136277\n",
      "weights: [0.00027167 0.00042204 0.00027167 ... 0.00026536 0.00027167 0.00091551]\n",
      "Step 8 / 20\n",
      "alpha: -0.04804616305362911\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00025291 0.00027167 0.00087256]\n",
      "Step 9 / 20\n",
      "alpha: 0.07035853430860603\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027135 0.00027167 0.00093617]\n",
      "Step 10 / 20\n",
      "alpha: -0.01808853083641948\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00026649 0.00027167 0.00091939]\n",
      "Step 11 / 20\n",
      "alpha: 0.017909815277232104\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.0002713  0.00027167 0.000936  ]\n",
      "Step 12 / 20\n",
      "alpha: -0.01773428774095342\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00026653 0.00027167 0.00091955]\n",
      "Step 13 / 20\n",
      "alpha: 0.017561861486047395\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 14 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 15 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 16 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 17 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 18 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 19 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 20 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00040224 0.00027167 ... 0.00027125 0.00027167 0.00093584]\n",
      "Step 1 / 20\n",
      "alpha: 2.2981108125991248\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00027167]\n",
      "Step 2 / 20\n",
      "alpha: 0.7673019049549871\n",
      "weights: [0.00027167 0.00058515 0.00027167 ... 0.00058515 0.00027167 0.00058515]\n",
      "Step 3 / 20\n",
      "alpha: 0.23750994904746237\n",
      "weights: [0.00027167 0.00074202 0.00027167 ... 0.00074202 0.00027167 0.00074202]\n",
      "Step 4 / 20\n",
      "alpha: 0.021229948241039655\n",
      "weights: [0.00027167 0.00075795 0.00027167 ... 0.00074202 0.00027167 0.00074202]\n",
      "Step 5 / 20\n",
      "alpha: -0.04142560812210892\n",
      "weights: [0.00027167 0.00072719 0.00027167 ... 0.00074202 0.00027167 0.00071191]\n",
      "Step 6 / 20\n",
      "alpha: -0.0901760706242543\n",
      "weights: [0.00027167 0.00066448 0.00027167 ... 0.00067804 0.00027167 0.00065053]\n",
      "Step 7 / 20\n",
      "alpha: 0.05544382375708855\n",
      "weights: [0.00027167 0.00070237 0.00027167 ... 0.0007167  0.00027167 0.00068761]\n",
      "Step 8 / 20\n",
      "alpha: 0.044359152719641956\n",
      "weights: [0.00027167 0.00073422 0.00027167 ... 0.0007492  0.00027167 0.00068761]\n",
      "Step 9 / 20\n",
      "alpha: -0.04161416963097607\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 10 / 20\n",
      "alpha: -4.440892098500627e-16\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 11 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 12 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 13 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 14 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 15 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 16 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 17 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 18 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 19 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 20 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.0007043  0.00027167 ... 0.00071867 0.00027167 0.00065958]\n",
      "Step 1 / 20\n",
      "alpha: 2.2405293628847023\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00027167]\n",
      "Step 2 / 20\n",
      "alpha: 0.9196684895480369\n",
      "weights: [0.00027167 0.00027167 0.00027167 ... 0.00027167 0.00027167 0.00068146]\n",
      "Step 3 / 20\n",
      "alpha: 0.4773312710713609\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00068146]\n",
      "Step 4 / 20\n",
      "alpha: -0.07930241952068959\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 5 / 20\n",
      "alpha: 8.881784197001248e-16\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 6 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 7 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 8 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 9 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 10 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 11 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 12 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 13 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 14 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 15 / 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 16 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 17 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 18 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 19 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 20 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027167 0.00043786 0.00027167 ... 0.00027167 0.00027167 0.00062951]\n",
      "Step 1 / 20\n",
      "alpha: 2.24335821908518\n",
      "weights: [0.00027174 0.00027174 0.00027174 ... 0.00027174 0.00256112 0.00027174]\n",
      "Step 2 / 20\n",
      "alpha: 0.9158514966608133\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00027174 0.00256112 0.00027174]\n",
      "Step 3 / 20\n",
      "alpha: 0.5205437020606252\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00045732 0.00256112 0.00045732]\n",
      "Step 4 / 20\n",
      "alpha: 0.10568537046683177\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.0005083  0.00256112 0.00045732]\n",
      "Step 5 / 20\n",
      "alpha: -0.07961956608400307\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.0004694  0.00256112 0.00045732]\n",
      "Step 6 / 20\n",
      "alpha: 0.022016148259704358\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00047985 0.00256112 0.00045732]\n",
      "Step 7 / 20\n",
      "alpha: -0.021694285449751244\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046955 0.00256112 0.00045732]\n",
      "Step 8 / 20\n",
      "alpha: 0.021379644341425152\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.0004797  0.00256112 0.00045732]\n",
      "Step 9 / 20\n",
      "alpha: -0.021071975224958312\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046969 0.00256112 0.00045732]\n",
      "Step 10 / 20\n",
      "alpha: 0.02077104007044424\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00047955 0.00256112 0.00045732]\n",
      "Step 11 / 20\n",
      "alpha: -0.020476611852681476\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 12 / 20\n",
      "alpha: 8.881784197001248e-16\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 13 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 14 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 15 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 16 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 17 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 18 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 19 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n",
      "Step 20 / 20\n",
      "alpha: 0.0\n",
      "weights: [0.00027174 0.00067905 0.00027174 ... 0.00046983 0.00256112 0.00045732]\n"
     ]
    }
   ],
   "source": [
    "# Perform the cross-validation\n",
    "\n",
    "tree_errors = []\n",
    "ada_errors = []\n",
    "num_ch = 5\n",
    "\n",
    "Xs, Ys = create_random_splits(X, Y, num_ch)\n",
    "\n",
    "for i in range(num_ch):\n",
    "    X_left_over = np.concatenate(Xs[:i] + Xs[i+1:], axis=0)\n",
    "    Y_left_over = np.concatenate(Ys[:i] + Ys[i+1:], axis=0)\n",
    "    X_ch = Xs[i]\n",
    "    Y_ch = Ys[i]\n",
    "\n",
    "    tree = DecisionTree(X_left_over, Y_left_over, n_0=400)\n",
    "    tree.train()\n",
    "    tree_clf = tree.predict\n",
    "    \n",
    "    clfs_coeffs = adaboost(X_left_over, Y_left_over, n_0=400)\n",
    "    ada_clf = sign_of_combo_clf(clfs_coeffs)\n",
    "    \n",
    "    Y_pred_tree = np.array(tree_clf(X_ch))\n",
    "    Y_pred_ada = np.array(ada_clf(X_ch))\n",
    "    \n",
    "    tree_error = (Y_pred_tree != Y_ch).mean()\n",
    "    ada_error = (Y_pred_ada != Y_ch).mean()\n",
    "    \n",
    "    tree_errors.append(tree_error)\n",
    "    ada_errors.append(ada_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b90f774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation errors for decision tree: [0.1, 0.09565217391304348, 0.10760869565217392, 0.10652173913043478, 0.09120521172638436]\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross-validation errors for decision tree: {tree_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d39f92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation errors for AdaBoost: [0.1, 0.09565217391304348, 0.10760869565217392, 0.10652173913043478, 0.09120521172638436]\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross-validation errors for AdaBoost: {ada_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01fea2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation error for decision tree: 0.10020\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean cross-validation error for decision tree: {np.mean(tree_errors):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "614043f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation error for AdaBoost tree: 0.10020\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean cross-validation error for AdaBoost tree: {np.mean(ada_errors):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09924403",
   "metadata": {},
   "source": [
    "In this case Adaboost didn't actually improve performance. I'll come back to this later. In particular, the algorithm for choosing a split can be sped up. Speeding up choosing a split and training trees with a smaller number of samples per leaf node might result in better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
