{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c97986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from numpy.linalg import inv, det\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import partial\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e679d",
   "metadata": {},
   "source": [
    "First let's prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d7a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spam.txt', 'r') as f:\n",
    "    arr = []\n",
    "    for line in f:\n",
    "        row = line.split()\n",
    "        row = list(map(lambda x: float(x), row))\n",
    "        arr.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ff6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7430ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[57] = df[57].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5a74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0559da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd568b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ds[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269ccdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbd83e",
   "metadata": {},
   "source": [
    "# Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cb8c6",
   "metadata": {},
   "source": [
    "## Part (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66044b5f",
   "metadata": {},
   "source": [
    "First we classify the data using LDA. The Python function below carries out LDA and returns the resulting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62033f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(X, Y):\n",
    "    \"\"\"\n",
    "        Construct the LDA classifier defined by explanatory variable X\n",
    "        and response variable Y and return it.\n",
    "        Returns: h, a Python function taking a row with length X.shape[0]\n",
    "        and returning a class 0 or 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The probabilities of the two classes 0 and 1\n",
    "    pi0 = (1-Y).mean()\n",
    "    pi1 = Y.mean()\n",
    "    \n",
    "    # The rows in X corresponding to the classes Y == 0\n",
    "    # and Y == 1, respectively\n",
    "    X0 = X[Y == 0.]\n",
    "    X1 = X[Y == 1.]\n",
    "    \n",
    "    # The number of examples in each class\n",
    "    n0 = len(X0)\n",
    "    n1 = len(X1)\n",
    "    \n",
    "    # The means of each feature in X taken over each\n",
    "    # class Y == 0 and Y == 1\n",
    "    mu0 = X0.mean(axis=0)\n",
    "    mu1 = X1.mean(axis=0)\n",
    "    \n",
    "    # The matrices defining the quadratic forms for QDA\n",
    "    mat0 = \\\n",
    "        np.array([(X0 - mu0)[i].reshape(-1,1) @ (X0 - mu0)[i].reshape(1,-1) for i in range(n0)])\n",
    "    mat1 = \\\n",
    "        np.array([(X1 - mu1)[i].reshape(-1,1) @ (X1 - mu1)[i].reshape(1,-1) for i in range(n1)])\n",
    "    S0 = mat0.mean(axis=0)\n",
    "    S1 = mat1.mean(axis=0)\n",
    "\n",
    "    # The averaged matrix S defining the quadratic form for LDA\n",
    "    S = (n0 * S0 + n1 * S1) / (n0 + n1)\n",
    "    \n",
    "    # The two quadratic forms used in LDA classification\n",
    "    delta0 = lambda x: x @ inv(S) @ mu0.T - 0.5 * mu0 @ inv(S) @ mu0.T + np.log(pi0)\n",
    "    delta1 = lambda x: x @ inv(S) @ mu1.T - 0.5 * mu1 @ inv(S) @ mu1.T + np.log(pi1)\n",
    "    \n",
    "    # The LDA classifier\n",
    "    h = lambda x: np.argmax([delta0(x), delta1(x)])\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630d33e",
   "metadata": {},
   "source": [
    "Below we construct an LDA classifier and use it to predict the response for $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca1b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LDA classifier\n",
    "h = LDA(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac03347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted classes from applying h to X\n",
    "Y_pred = np.array(list(map(h, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0342631",
   "metadata": {},
   "source": [
    "Below we display the table with entries $2x2$ table specified in the problem statement and compute the overall misclassification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7cd6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(Y, Y_pred):\n",
    "    \"\"\"\n",
    "        Compute a 2x2 matrix with (i,j) entry\n",
    "        equal to the number of entries with\n",
    "        Y == i and Y_pred == j.\n",
    "    \"\"\"\n",
    "    \n",
    "    m00 = ((Y == 0) & (Y_pred == 0)).sum()\n",
    "    m01 = ((Y == 0) & (Y_pred == 1)).sum()\n",
    "    m10 = ((Y == 1) & (Y_pred == 0)).sum()\n",
    "    m11 = ((Y == 1) & (Y_pred == 1)).sum()\n",
    "    \n",
    "    return np.array([[m00, m01],[m10, m11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c869e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for LDA: \n",
      "[[2663  125]\n",
      " [ 387 1426]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Table for LDA: \\n{get_table(Y, Y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f425c7",
   "metadata": {},
   "source": [
    "|&nbsp;|$\\hat h(x) = 0$| $\\hat h(x) = 1$|\n",
    "| :--- | :--- | :--- |\n",
    "|$Y=0$|2663|125|\n",
    "|$Y=1$|387|1426|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7bf24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "((m00, m01), (m10,m11)) = get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c54ac4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA misclassification rate: 0.11128\n"
     ]
    }
   ],
   "source": [
    "mis = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "print(f'LDA misclassification rate: {mis:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76fef1",
   "metadata": {},
   "source": [
    "## Part (ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aca498",
   "metadata": {},
   "source": [
    "Now we classify the data using QDA. The Python function below carries out QDA and returns the resulting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7401a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QDA(X, Y):\n",
    "    \"\"\"\n",
    "        Construct the LDA classifier defined by explanatory variable X\n",
    "        and response variable Y and return it.\n",
    "        Returns: h, a Python function taking a row with length X.shape[0]\n",
    "        and returning a class 0 or 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The probabilities of the two classes 0 and 1\n",
    "    pi0 = (1-Y).mean()\n",
    "    pi1 = Y.mean()\n",
    "\n",
    "    # The rows in X corresponding to the classes Y == 0\n",
    "    # and Y == 1, respectively\n",
    "    X0 = X[Y == 0.]\n",
    "    X1 = X[Y == 1.]\n",
    "\n",
    "    # The number of examples in each class\n",
    "    n0 = len(X0)\n",
    "    n1 = len(X1)\n",
    "    \n",
    "    # The means of each feature in X taken over each\n",
    "    # class Y == 0 and Y == 1\n",
    "    mu0 = X0.mean(axis=0)\n",
    "    mu1 = X1.mean(axis=0)\n",
    "\n",
    "    # The matrices defining the quadratic forms for QDA\n",
    "    mat0 = \\\n",
    "        np.array([(X0 - mu0)[i].reshape(-1,1) @ (X0 - mu0)[i].reshape(1,-1) for i in range(n0)])\n",
    "    mat1 = \\\n",
    "        np.array([(X1 - mu1)[i].reshape(-1,1) @ (X1 - mu1)[i].reshape(1,-1) for i in range(n1)])\n",
    "    S0 = mat0.mean(axis=0)\n",
    "    S1 = mat1.mean(axis=0)\n",
    "\n",
    "    # The two quadratic forms used in QDA classification\n",
    "    delta0 = lambda x: -0.5 * np.log(det(S0)) - 0.5 * (x - mu0) @ inv(S0) @ (x - mu0).T + np.log(pi0)\n",
    "    delta1 = lambda x: -0.5 * np.log(det(S1)) - 0.5 * (x - mu1) @ inv(S1) @ (x - mu1).T + np.log(pi1)\n",
    "\n",
    "    # The QDA classifier\n",
    "    h = lambda x: np.argmax([delta0(x), delta1(x)])\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73a7f7",
   "metadata": {},
   "source": [
    "Again we construct the QDA classifier and predict $Y$ using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab47979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The QDA classifier\n",
    "h = QDA(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f8e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted Y values\n",
    "Y_pred = np.array(list(map(h, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea6d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for QDA: \n",
      "[[2101  687]\n",
      " [  82 1731]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Table for QDA: \\n{get_table(Y, Y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e79864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "((m00, m01), (m10, m1)) = get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd5156",
   "metadata": {},
   "source": [
    "|&nbsp;|$\\hat h(x) = 0$| $\\hat h(x) = 1$|\n",
    "| :--- | :--- | :--- |\n",
    "|$Y=0$|2101|687|\n",
    "|$Y=1$|82|1731|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b0cd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA misclassification rate: 0.17900\n"
     ]
    }
   ],
   "source": [
    "mis = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "print(f'QDA misclassification rate: {mis:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b9270",
   "metadata": {},
   "source": [
    "## Part (iii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f4e86",
   "metadata": {},
   "source": [
    "Now we try logistic regression, using scikit-learn's built-in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeacdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A logistic regression model\n",
    "clf = LogisticRegression(penalty=None, fit_intercept=True,\n",
    "                         random_state=42, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19864bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, penalty=None, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, penalty=None, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, penalty=None, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the data\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c843e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Y values\n",
    "Y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fc2a8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for logistic regression: \n",
      "[[2665  123]\n",
      " [ 194 1619]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Table for logistic regression: \\n{get_table(Y, Y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336c484",
   "metadata": {},
   "source": [
    "|&nbsp;|$\\hat h(x) = 0$| $\\hat h(x) = 1$|\n",
    "| :--- | :--- | :--- |\n",
    "|$Y=0$|2665|123|\n",
    "|$Y=1$|194|1619|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5886485",
   "metadata": {},
   "outputs": [],
   "source": [
    "((m00, m01), (m10, m11)) = get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9a1a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression misclassification rate: 0.06890\n"
     ]
    }
   ],
   "source": [
    "mis = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "print(f'Logistic regression misclassification rate: {mis:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47a07a",
   "metadata": {},
   "source": [
    "## Part (iv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87790b",
   "metadata": {},
   "source": [
    "Below we train a decision tree on the pair $(X,Y)$. We'll do this from scratch which necessitates defining some helper functions and a class to handle the classification. Additionally, we allow weights on the different examples in the dataset, in order to handle the general setting of AdaBoost later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b661ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_with_weights(Y, weights, indices):\n",
    "    \"\"\"\n",
    "        Compute the (weighted) proportion of classes\n",
    "        from the rows specified by indices in the dataset (X,Y)\n",
    "        Y: classes for all rows of the dataset\n",
    "        weights: an array with shape equal to Y.shape\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        Returns:\n",
    "            p0: proportion in class 0\n",
    "            p1: proportion in class 1\n",
    "    \"\"\"\n",
    "    p0 = np.dot(Y[indices] == 0, weights[indices]) / len(Y[indices])\n",
    "    p1 = np.dot(Y[indices] == 1, weights[indices]) / len(Y[indices])\n",
    "    \n",
    "    return p0, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea8e5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_indices(X, Y, indices, col, t):\n",
    "    \"\"\"\n",
    "        Calculate new indices obtained by splitting on the column col\n",
    "        according to col <= t or col > t. The new indices are obtained\n",
    "        by restricting (X,Y) to the rows specified by indices.\n",
    "        X, Y: arrays with X.shape[0] = Y.shape[0]\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        col: the column specifying a feature to split on\n",
    "        t: the value defining the split according to col <= or col > t\n",
    "        Returns:\n",
    "            new_indices: a pair of subsets of indices which\n",
    "            partition indices when taken together\n",
    "    \"\"\"\n",
    "    new_indices = (np.intersect1d(indices, np.where(X[:, col] <= t)),\n",
    "            np.intersect1d(indices, np.where(X[:, col] > t)))\n",
    "    return new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552abd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_impurity_with_weights(X, Y, weights, indices, col, t):\n",
    "    \"\"\"\n",
    "        Calculate the Gini impurity (with weights) after splitting (X,Y)\n",
    "        on the column col according to col <= t or col > t.\n",
    "        X, Y: arrays with X.shape[0] = Y.shape[0]\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        weights: an array with weights.shape = Y.shape\n",
    "        col: the column specifying a feature to split on\n",
    "        t: the value defining the split according to col <= or col > t\n",
    "    \"\"\"\n",
    "    \n",
    "    new_indices = split_to_indices(X, Y, indices, col, t)\n",
    "    \n",
    "    gammas = []\n",
    "    \n",
    "    for new_ind in new_indices:\n",
    "        p0, p1 = proportion_with_weights(Y, weights, new_ind)\n",
    "        gamma = 1 - p0**2 - p1**2\n",
    "        gammas.append(gamma)\n",
    "    \n",
    "    impurity = np.array(gammas).sum()\n",
    "    \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b3d7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_with_weights(X, Y, indices, weights, n_0=500, frac=4, num_ts=20):\n",
    "    \"\"\"\n",
    "        Calculate a split to minimize the gini impurity.\n",
    "        X, Y: arrays with X.shape[0] = Y.shape[0]\n",
    "        indices: a subset of the indices from 0 to Y.shape[0]\n",
    "        weights: an array with weights.shape = Y.shape\n",
    "        n_0: do not compute a split if there are fewer than this\n",
    "            number of indices in indices\n",
    "        frac: only create a split in which both subsets of the\n",
    "            partition of indices contain at least n_0 / frac elements\n",
    "        num_ts: the number of values of t to test as in col <= t\n",
    "            and col < t\n",
    "        Returns:\n",
    "            best_split: a partition of indices according to the best\n",
    "            split\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(indices) <= n_0:\n",
    "        return (None, None)\n",
    "    else:\n",
    "        best_split = (None, None)\n",
    "        min_impurity = float('inf')\n",
    "\n",
    "        k = X.shape[1]\n",
    "        for col in range(k):\n",
    "            low = X[indices, col].min()\n",
    "            high = X[indices, col].max()\n",
    "            ts = np.linspace(low, high, num_ts)\n",
    "\n",
    "            for t in ts:\n",
    "                impurity = calc_impurity_with_weights(X, Y, weights, indices, col, t)\n",
    "                new_indices = split_to_indices(X, Y, indices, col, t)\n",
    "                if len(new_indices[0]) >= n_0 / frac and len(new_indices[1]) >= n_0 / frac:\n",
    "                    if impurity < min_impurity:\n",
    "                        min_impurity = impurity\n",
    "                        best_split = (col, t)\n",
    "\n",
    "        return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "845a8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "        Represents a node of a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y, indices, col=None, t=None, lchild=None, rchild=None):\n",
    "        \"\"\"\n",
    "            (X, Y): the dataset to train on\n",
    "            indices: the indices corresponding to the split\n",
    "                represented by this node\n",
    "            col: a column to split on, if the node has children\n",
    "            t: the value for the split according to col <= t or col > t\n",
    "            lchild, rchild: nodes representing the left and right child\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.avg = Y[indices].mean()\n",
    "        self.indices = indices\n",
    "        self.col = col\n",
    "        self.t = t\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "        \n",
    "        # Whether this node is a leaf or not\n",
    "        self.is_leaf = True\n",
    "    \n",
    "    def split(self, weights, n_0, frac, num_ts):\n",
    "        \"\"\"\n",
    "            Split the node using generate_split_with_weights and assign\n",
    "            its children, column, and t\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            (col, t) = generate_split_with_weights(self.X, self.Y, self.indices, weights, n_0, frac, num_ts)\n",
    "            if col is not None:\n",
    "                new_indices = split_to_indices(self.X, self.Y, self.indices, col, t)\n",
    "                self.col = col\n",
    "                self.t = t\n",
    "                self.lchild = Node(self.X, self.Y, new_indices[0])\n",
    "                self.rchild = Node(self.X, self.Y, new_indices[1])\n",
    "            self.is_leaf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bd87e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "        Represents a decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y, weights=np.ones(len(Y)) / len(Y), n_0=500, frac=4, num_ts=20):\n",
    "        \"\"\"\n",
    "            Initialize the tree with a single node.\n",
    "            (X, Y): the dataset to train on\n",
    "            weights: an array with weights.shape=Y.shape representing\n",
    "                weights for the different examples of the dataset\n",
    "            n_0: an integer such that nodes of size <= n_0 are not split\n",
    "                further\n",
    "            frac: an integer such that each node has size >= n_0/frac\n",
    "            num_ts: the number of values of t to test for each column\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.weights = weights      \n",
    "        self.n_0 = n_0\n",
    "        self.frac = frac\n",
    "        self.num_ts = num_ts\n",
    "        \n",
    "        # Initialize the tree with a single node\n",
    "        # representing all indices of the dataset.\n",
    "        # The children of node i in self.tree\n",
    "        # lie at indices 2*i+1 and 2*i+2.\n",
    "        self.root = Node(X, Y, list(range(len(X))))\n",
    "        self.tree = [self.root]\n",
    "        \n",
    "        self.num_leaves = 1\n",
    "    \n",
    "    def split_leaves(self):\n",
    "        \"\"\"\n",
    "            Split each leaf node.\n",
    "            Returns:\n",
    "                add_new_nodes: a Boolean saying whether any new\n",
    "                nodes were created by splitting or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # An array to hold the new children of the leaves\n",
    "        children = []\n",
    "        \n",
    "        # Split each node. If the node has been split already\n",
    "        # it won't be split again.\n",
    "        for node in self.tree[-self.num_leaves:]:\n",
    "            \n",
    "            # Add the children created by the split for each leaf node.\n",
    "            # If the node is None then its children will be None.\n",
    "            if node is not None:\n",
    "                node.split(self.weights, self.n_0, self.frac, self.num_ts)\n",
    "                children += [node.lchild, node.rchild]\n",
    "        \n",
    "        # A boolean recording whether any new children have been created\n",
    "        add_new_nodes = any(children)\n",
    "        \n",
    "        # If there are new children, add them to the tree.\n",
    "        if add_new_nodes:\n",
    "            self.tree += children\n",
    "            self.num_leaves = len(children)\n",
    "        \n",
    "        return add_new_nodes\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            Split all leaf nodes until none can be split further\n",
    "            according to our end condition.\n",
    "        \"\"\"\n",
    "        continue_splitting = True\n",
    "        \n",
    "        while continue_splitting:\n",
    "            continue_splitting = self.split_leaves()\n",
    "    \n",
    "    def predict_flt_one(self, x):\n",
    "        \"\"\"\n",
    "            Predict a float for the array x of shape\n",
    "            X.shape[1]. This float will be the average\n",
    "            of the classes for the node in which x lies\n",
    "            in the decision tree.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        \n",
    "        \n",
    "        # Traverse the decision tree until coming to the\n",
    "        # node which contains x.\n",
    "        while node.col:\n",
    "            if x[node.col] > node.t:\n",
    "                node = node.rchild\n",
    "            else:\n",
    "                node = node.lchild\n",
    "        \n",
    "        return node.avg\n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        \"\"\"\n",
    "            Predict a class for the array x of\n",
    "            shape X.shape[1].\n",
    "        \"\"\"\n",
    "        return np.sign(self.predict_flt_one(x))\n",
    "    \n",
    "    def predict_flt(self, x):\n",
    "        \"\"\"\n",
    "            Predict floats for the array x with\n",
    "            x.shape[1] = X.shape[1].\n",
    "        \"\"\"\n",
    "        return np.array(list(map(self.predict_flt_one, x)))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "            Predict classes for the array x with\n",
    "            x.shape[1] = X.shape[1].\n",
    "        \"\"\"\n",
    "        return np.array(list(map(self.predict_one, x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28eeebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTree(X, Y, n_0=20, num_ts=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26870c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/7qb3ygvs0mzb0cdjbh8z8svr0000gn/T/ipykernel_78032/4096217768.py:12: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p0 = np.dot(Y[indices] == 0, weights[indices]) / len(Y[indices])\n",
      "/var/folders/h9/7qb3ygvs0mzb0cdjbh8z8svr0000gn/T/ipykernel_78032/4096217768.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p1 = np.dot(Y[indices] == 1, weights[indices]) / len(Y[indices])\n"
     ]
    }
   ],
   "source": [
    "clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53da89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce66d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for decision tree: \n",
      "[[1672 1116]\n",
      " [   0 1813]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Table for decision tree: \\n{get_table(Y, Y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ddbe1",
   "metadata": {},
   "source": [
    "|&nbsp;|$\\hat h(x) = 0$| $\\hat h(x) = 1$|\n",
    "| :--- | :--- | :--- |\n",
    "|$Y=0$|1672|1116|\n",
    "|$Y=1$|0|1813|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "beb58995",
   "metadata": {},
   "outputs": [],
   "source": [
    "((m00, m01), (m10, m11)) = get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d38598d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree misclassification rate: 0.24256\n"
     ]
    }
   ],
   "source": [
    "mis = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "print(f'Tree misclassification rate: {mis:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce6631",
   "metadata": {},
   "source": [
    "# Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7214e",
   "metadata": {},
   "source": [
    "Below we create a couple of functions to create the chunks for cross-validation and ultimately perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "911b66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_splits(X, Y, num_ch):\n",
    "    \"\"\"\n",
    "        Split the dataset (X, Y) into num_ch random chunks\n",
    "        of equal size (except for possibly the last chunk).\n",
    "        (X, Y): the dataset\n",
    "        num_ch: the number of chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly permute (X, Y)\n",
    "    n = len(X)\n",
    "    indices = np.random.permutation(n)\n",
    "    X_perm = X[indices]\n",
    "    Y_perm = Y[indices]\n",
    "    \n",
    "    k = n // num_ch\n",
    "    \n",
    "    # Arrays to hold the chunks\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    \n",
    "    # Append one chunk at a time\n",
    "    for i in range(num_ch):\n",
    "        if i < num_ch-1:\n",
    "            Xs.append(X_perm[i*k:i*k + k, :])\n",
    "            Ys.append(Y_perm[i*k:i*k + k])\n",
    "        else:\n",
    "            Xs.append(X_perm[i*k:, :])\n",
    "            Ys.append(Y_perm[i*k:])\n",
    "    \n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea54b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, Y, num_ch, model_name):\n",
    "    \"\"\"\n",
    "        Perform n-fold cross-validation where\n",
    "        (X, Y): the dataset\n",
    "        num_ch: the number of folds\n",
    "        model_name: either 'LDA' or 'log_reg'\n",
    "        Returns:\n",
    "            errors: a list of errors for each fold\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    Xs, Ys = create_random_splits(X, Y, num_ch)\n",
    "    \n",
    "    # Train the model for each fold and compute\n",
    "    # the error on the held out test set.\n",
    "    for i in range(num_ch):\n",
    "        \n",
    "        # (X_left_over, Y_left_over) is the training dataset\n",
    "        # for this fold.\n",
    "        # (X_ch, Y_ch) is the held out test set.\n",
    "        X_left_over = np.concatenate(Xs[:i] + Xs[i+1:], axis=0)\n",
    "        Y_left_over = np.concatenate(Ys[:i] + Ys[i+1:], axis=0)\n",
    "        X_ch = Xs[i]\n",
    "        Y_ch = Ys[i]\n",
    "        \n",
    "        if model_name == 'LDA':\n",
    "            h = LDA(X_left_over, Y_left_over)\n",
    "            Y_pred = np.array(list(map(h, X_ch))) \n",
    "            ((m00, m01), (m10, m11)) = get_table(Y_ch, Y_pred)\n",
    "            misc = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "            errors.append(misc)\n",
    "            \n",
    "        elif model_name == 'log_reg':\n",
    "            clf = LogisticRegression(penalty=None, fit_intercept=True,\n",
    "                         random_state=42, max_iter=10000)\n",
    "            clf.fit(X_left_over, Y_left_over)\n",
    "            Y_pred = clf.predict(X_ch)\n",
    "            ((m00, m01), (m10, m11)) = get_table(Y_ch, Y_pred)\n",
    "            misc = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "            errors.append(misc)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a4421",
   "metadata": {},
   "source": [
    "We report the CV errors below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83261517",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_errors = cross_validation(X, Y, 5, 'LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4213284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA errors under 5-fold cross validation: \n",
      " [0.11847826086956521, 0.10652173913043478, 0.11956521739130435, 0.10652173913043478, 0.11183496199782844]\n"
     ]
    }
   ],
   "source": [
    "print(f'LDA errors under 5-fold cross validation: \\n {LDA_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ad0f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA cross validation error: 0.11258\n"
     ]
    }
   ],
   "source": [
    "print(f'LDA cross validation error: {np.mean(LDA_errors):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02c80896",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_errors = cross_validation(X, Y, 5, 'log_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1083b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression errors under 5-fold cross validation: \n",
      " [0.06956521739130435, 0.07065217391304347, 0.09347826086956522, 0.07282608695652174, 0.05537459283387622]\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic regression errors under 5-fold cross validation: \\n {log_errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b28af3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression cross validation error: 0.07238\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic regression cross validation error: {np.mean(log_errors):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f6177",
   "metadata": {},
   "source": [
    "It appears that logistic regression generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa6734",
   "metadata": {},
   "source": [
    "# Part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599d15e",
   "metadata": {},
   "source": [
    "Now we compare means across the different variables for the two different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "befef57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a000952",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spam = X[Y == 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4c13dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = X[Y == 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98d3bc",
   "metadata": {},
   "source": [
    "The cell below reports p-values for the Wald test on difference of means between the spam and normal emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f75ee2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariate 0\n",
      "observed mean for spam emails: 0.1523\n",
      "Observed mean for normal emails: 0.0735\n",
      "Estimated standard error: 0.0028\n",
      "Wald statistic: 27.9665\n",
      "Estimated p-value: 4.1564769180713155e-172\n",
      "\n",
      "Covariate 1\n",
      "observed mean for spam emails: 0.1646\n",
      "Observed mean for normal emails: 0.2445\n",
      "Estimated standard error: 0.0506\n",
      "Wald statistic: -1.5780\n",
      "Estimated p-value: 0.1145663100599748\n",
      "\n",
      "Covariate 2\n",
      "observed mean for spam emails: 0.4038\n",
      "Observed mean for normal emails: 0.2006\n",
      "Estimated standard error: 0.0072\n",
      "Wald statistic: 28.0833\n",
      "Estimated p-value: 1.565013197321854e-173\n",
      "\n",
      "Covariate 3\n",
      "observed mean for spam emails: 0.1647\n",
      "Observed mean for normal emails: 0.0009\n",
      "Estimated standard error: 0.1156\n",
      "Wald statistic: 1.4170\n",
      "Estimated p-value: 0.15648638835252\n",
      "\n",
      "Covariate 4\n",
      "observed mean for spam emails: 0.5140\n",
      "Observed mean for normal emails: 0.1810\n",
      "Estimated standard error: 0.0137\n",
      "Wald statistic: 24.2208\n",
      "Estimated p-value: 1.341904234721225e-129\n",
      "\n",
      "Covariate 5\n",
      "observed mean for spam emails: 0.1749\n",
      "Observed mean for normal emails: 0.0445\n",
      "Estimated standard error: 0.0026\n",
      "Wald statistic: 49.9714\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 6\n",
      "observed mean for spam emails: 0.2754\n",
      "Observed mean for normal emails: 0.0094\n",
      "Estimated standard error: 0.0077\n",
      "Wald statistic: 34.6099\n",
      "Estimated p-value: 1.791305257894461e-262\n",
      "\n",
      "Covariate 7\n",
      "observed mean for spam emails: 0.2081\n",
      "Observed mean for normal emails: 0.0384\n",
      "Estimated standard error: 0.0071\n",
      "Wald statistic: 24.0273\n",
      "Estimated p-value: 1.4425874690691554e-127\n",
      "\n",
      "Covariate 8\n",
      "observed mean for spam emails: 0.1701\n",
      "Observed mean for normal emails: 0.0380\n",
      "Estimated standard error: 0.0030\n",
      "Wald statistic: 43.3165\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 9\n",
      "observed mean for spam emails: 0.3505\n",
      "Observed mean for normal emails: 0.1672\n",
      "Estimated standard error: 0.0122\n",
      "Wald statistic: 15.0245\n",
      "Estimated p-value: 5.0744590654092484e-51\n",
      "\n",
      "Covariate 10\n",
      "observed mean for spam emails: 0.1184\n",
      "Observed mean for normal emails: 0.0217\n",
      "Estimated standard error: 0.0015\n",
      "Wald statistic: 62.8885\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 11\n",
      "observed mean for spam emails: 0.5500\n",
      "Observed mean for normal emails: 0.5363\n",
      "Estimated standard error: 0.0205\n",
      "Wald statistic: 0.6643\n",
      "Estimated p-value: 0.5065219222463955\n",
      "\n",
      "Covariate 12\n",
      "observed mean for spam emails: 0.1435\n",
      "Observed mean for normal emails: 0.0617\n",
      "Estimated standard error: 0.0031\n",
      "Wald statistic: 25.9995\n",
      "Estimated p-value: 5.014652223836088e-149\n",
      "\n",
      "Covariate 13\n",
      "observed mean for spam emails: 0.0836\n",
      "Observed mean for normal emails: 0.0424\n",
      "Estimated standard error: 0.0033\n",
      "Wald statistic: 12.5615\n",
      "Estimated p-value: 3.438691864471536e-36\n",
      "\n",
      "Covariate 14\n",
      "observed mean for spam emails: 0.1121\n",
      "Observed mean for normal emails: 0.0083\n",
      "Estimated standard error: 0.0035\n",
      "Wald statistic: 29.4618\n",
      "Estimated p-value: 8.886679165184924e-191\n",
      "\n",
      "Covariate 15\n",
      "observed mean for spam emails: 0.5184\n",
      "Observed mean for normal emails: 0.0736\n",
      "Estimated standard error: 0.0251\n",
      "Wald statistic: 17.6871\n",
      "Estimated p-value: 5.275563010390062e-70\n",
      "\n",
      "Covariate 16\n",
      "observed mean for spam emails: 0.2875\n",
      "Observed mean for normal emails: 0.0483\n",
      "Estimated standard error: 0.0093\n",
      "Wald statistic: 25.8528\n",
      "Estimated p-value: 2.264060019756424e-147\n",
      "\n",
      "Covariate 17\n",
      "observed mean for spam emails: 0.3192\n",
      "Observed mean for normal emails: 0.0973\n",
      "Estimated standard error: 0.0108\n",
      "Wald statistic: 20.4840\n",
      "Estimated p-value: 2.990920523837692e-93\n",
      "\n",
      "Covariate 18\n",
      "observed mean for spam emails: 2.2645\n",
      "Observed mean for normal emails: 1.2703\n",
      "Estimated standard error: 0.0838\n",
      "Wald statistic: 11.8570\n",
      "Estimated p-value: 1.9794576693188454e-32\n",
      "\n",
      "Covariate 19\n",
      "observed mean for spam emails: 0.2055\n",
      "Observed mean for normal emails: 0.0076\n",
      "Estimated standard error: 0.0146\n",
      "Wald statistic: 13.5663\n",
      "Estimated p-value: 6.345253358431158e-42\n",
      "\n",
      "Covariate 20\n",
      "observed mean for spam emails: 1.3804\n",
      "Observed mean for normal emails: 0.4387\n",
      "Estimated standard error: 0.0406\n",
      "Wald statistic: 23.2084\n",
      "Estimated p-value: 3.743054431009823e-119\n",
      "\n",
      "Covariate 21\n",
      "observed mean for spam emails: 0.2380\n",
      "Observed mean for normal emails: 0.0452\n",
      "Estimated standard error: 0.0492\n",
      "Wald statistic: 3.9165\n",
      "Estimated p-value: 8.983872910641739e-05\n",
      "\n",
      "Covariate 22\n",
      "observed mean for spam emails: 0.2471\n",
      "Observed mean for normal emails: 0.0071\n",
      "Estimated standard error: 0.0063\n",
      "Wald statistic: 37.9275\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 23\n",
      "observed mean for spam emails: 0.2129\n",
      "Observed mean for normal emails: 0.0171\n",
      "Estimated standard error: 0.0086\n",
      "Wald statistic: 22.8104\n",
      "Estimated p-value: 3.619200348978371e-115\n",
      "\n",
      "Covariate 24\n",
      "observed mean for spam emails: 0.0175\n",
      "Observed mean for normal emails: 0.8955\n",
      "Estimated standard error: 0.0812\n",
      "Wald statistic: -10.8102\n",
      "Estimated p-value: 3.0809583736177965e-27\n",
      "\n",
      "Covariate 25\n",
      "observed mean for spam emails: 0.0092\n",
      "Observed mean for normal emails: 0.4320\n",
      "Estimated standard error: 0.0231\n",
      "Wald statistic: -18.2821\n",
      "Estimated p-value: 1.1498956824834331e-74\n",
      "\n",
      "Covariate 26\n",
      "observed mean for spam emails: 0.0015\n",
      "Observed mean for normal emails: 1.2653\n",
      "Estimated standard error: 0.3424\n",
      "Wald statistic: -3.6910\n",
      "Estimated p-value: 0.00022335758554222087\n",
      "\n",
      "Covariate 27\n",
      "observed mean for spam emails: 0.0188\n",
      "Observed mean for normal emails: 0.1938\n",
      "Estimated standard error: 0.0080\n",
      "Wald statistic: -21.8888\n",
      "Estimated p-value: 3.3211675655462475e-106\n",
      "\n",
      "Covariate 28\n",
      "observed mean for spam emails: 0.0007\n",
      "Observed mean for normal emails: 0.1628\n",
      "Estimated standard error: 0.0108\n",
      "Wald statistic: -15.0071\n",
      "Estimated p-value: 6.600695385809589e-51\n",
      "\n",
      "Covariate 29\n",
      "observed mean for spam emails: 0.0060\n",
      "Observed mean for normal emails: 0.1659\n",
      "Estimated standard error: 0.0062\n",
      "Wald statistic: -25.7883\n",
      "Estimated p-value: 1.1991353813239879e-146\n",
      "\n",
      "Covariate 30\n",
      "observed mean for spam emails: 0.0013\n",
      "Observed mean for normal emails: 0.1060\n",
      "Estimated standard error: 0.0050\n",
      "Wald statistic: -21.0051\n",
      "Estimated p-value: 5.892570090167313e-98\n",
      "\n",
      "Covariate 31\n",
      "observed mean for spam emails: 0.0005\n",
      "Observed mean for normal emails: 0.0773\n",
      "Estimated standard error: 0.0033\n",
      "Wald statistic: -23.0856\n",
      "Estimated p-value: 6.465788715470687e-118\n",
      "\n",
      "Covariate 32\n",
      "observed mean for spam emails: 0.0146\n",
      "Observed mean for normal emails: 0.1510\n",
      "Estimated standard error: 0.0094\n",
      "Wald statistic: -14.5635\n",
      "Estimated p-value: 4.794811092761672e-48\n",
      "\n",
      "Covariate 33\n",
      "observed mean for spam emails: 0.0018\n",
      "Observed mean for normal emails: 0.0778\n",
      "Estimated standard error: 0.0033\n",
      "Wald statistic: -22.8087\n",
      "Estimated p-value: 3.760492542953251e-115\n",
      "\n",
      "Covariate 34\n",
      "observed mean for spam emails: 0.0069\n",
      "Observed mean for normal emails: 0.1695\n",
      "Estimated standard error: 0.0086\n",
      "Wald statistic: -18.8930\n",
      "Estimated p-value: 1.3032129045852809e-79\n",
      "\n",
      "Covariate 35\n",
      "observed mean for spam emails: 0.0295\n",
      "Observed mean for normal emails: 0.1417\n",
      "Estimated standard error: 0.0047\n",
      "Wald statistic: -23.7072\n",
      "Estimated p-value: 3.037965894299001e-124\n",
      "\n",
      "Covariate 36\n",
      "observed mean for spam emails: 0.0435\n",
      "Observed mean for normal emails: 0.1977\n",
      "Estimated standard error: 0.0048\n",
      "Wald statistic: -31.8523\n",
      "Estimated p-value: 1.2248321992919383e-222\n",
      "\n",
      "Covariate 37\n",
      "observed mean for spam emails: 0.0047\n",
      "Observed mean for normal emails: 0.0187\n",
      "Estimated standard error: 0.0015\n",
      "Wald statistic: -9.4050\n",
      "Estimated p-value: 5.204192380543365e-21\n",
      "\n",
      "Covariate 38\n",
      "observed mean for spam emails: 0.0124\n",
      "Observed mean for normal emails: 0.1217\n",
      "Estimated standard error: 0.0057\n",
      "Wald statistic: -19.1023\n",
      "Estimated p-value: 2.414451384933573e-81\n",
      "\n",
      "Covariate 39\n",
      "observed mean for spam emails: 0.0367\n",
      "Observed mean for normal emails: 0.0831\n",
      "Estimated standard error: 0.0036\n",
      "Wald statistic: -13.0083\n",
      "Estimated p-value: 1.0981588049584657e-38\n",
      "\n",
      "Covariate 40\n",
      "observed mean for spam emails: 0.0001\n",
      "Observed mean for normal emails: 0.0720\n",
      "Estimated standard error: 0.0040\n",
      "Wald statistic: -17.8229\n",
      "Estimated p-value: 4.690808449722528e-71\n",
      "\n",
      "Covariate 41\n",
      "observed mean for spam emails: 0.0024\n",
      "Observed mean for normal emails: 0.2168\n",
      "Estimated standard error: 0.0180\n",
      "Wald statistic: -11.8947\n",
      "Estimated p-value: 1.2611314502108945e-32\n",
      "\n",
      "Covariate 42\n",
      "observed mean for spam emails: 0.0085\n",
      "Observed mean for normal emails: 0.0706\n",
      "Estimated standard error: 0.0015\n",
      "Wald statistic: -41.2409\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 43\n",
      "observed mean for spam emails: 0.0062\n",
      "Observed mean for normal emails: 0.1266\n",
      "Estimated standard error: 0.0119\n",
      "Wald statistic: -10.0878\n",
      "Estimated p-value: 6.255703952905121e-24\n",
      "\n",
      "Covariate 44\n",
      "observed mean for spam emails: 0.1251\n",
      "Observed mean for normal emails: 0.4158\n",
      "Estimated standard error: 0.0302\n",
      "Wald statistic: -9.6348\n",
      "Estimated p-value: 5.702389300842863e-22\n",
      "\n",
      "Covariate 45\n",
      "observed mean for spam emails: 0.0147\n",
      "Observed mean for normal emails: 0.2872\n",
      "Estimated standard error: 0.0252\n",
      "Wald statistic: -10.8252\n",
      "Estimated p-value: 2.6139471553286943e-27\n",
      "\n",
      "Covariate 46\n",
      "observed mean for spam emails: 0.0012\n",
      "Observed mean for normal emails: 0.0082\n",
      "Estimated standard error: 0.0002\n",
      "Wald statistic: -39.3095\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 47\n",
      "observed mean for spam emails: 0.0021\n",
      "Observed mean for normal emails: 0.0512\n",
      "Estimated standard error: 0.0025\n",
      "Wald statistic: -19.4602\n",
      "Estimated p-value: 2.388069454274413e-84\n",
      "\n",
      "Covariate 48\n",
      "observed mean for spam emails: 0.0206\n",
      "Observed mean for normal emails: 0.0503\n",
      "Estimated standard error: 0.0018\n",
      "Wald statistic: -16.9419\n",
      "Estimated p-value: 2.2081886632837278e-64\n",
      "\n",
      "Covariate 49\n",
      "observed mean for spam emails: 0.1090\n",
      "Observed mean for normal emails: 0.1586\n",
      "Estimated standard error: 0.0023\n",
      "Wald statistic: -21.8716\n",
      "Estimated p-value: 4.847587942714766e-106\n",
      "\n",
      "Covariate 50\n",
      "observed mean for spam emails: 0.0082\n",
      "Observed mean for normal emails: 0.0227\n",
      "Estimated standard error: 0.0003\n",
      "Wald statistic: -41.5410\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 51\n",
      "observed mean for spam emails: 0.5137\n",
      "Observed mean for normal emails: 0.1100\n",
      "Estimated standard error: 0.0182\n",
      "Wald statistic: 22.1671\n",
      "Estimated p-value: 7.134979863809027e-109\n",
      "\n",
      "Covariate 52\n",
      "observed mean for spam emails: 0.1745\n",
      "Observed mean for normal emails: 0.0116\n",
      "Estimated standard error: 0.0031\n",
      "Wald statistic: 53.3600\n",
      "Estimated p-value: 0.0\n",
      "\n",
      "Covariate 53\n",
      "observed mean for spam emails: 0.0789\n",
      "Observed mean for normal emails: 0.0217\n",
      "Estimated standard error: 0.0089\n",
      "Wald statistic: 6.4507\n",
      "Estimated p-value: 1.1135299980496553e-10\n",
      "\n",
      "Covariate 54\n",
      "observed mean for spam emails: 9.5192\n",
      "Observed mean for normal emails: 2.3773\n",
      "Estimated standard error: 58.3232\n",
      "Wald statistic: 0.1225\n",
      "Estimated p-value: 0.9025400100032499\n",
      "\n",
      "Covariate 55\n",
      "observed mean for spam emails: 104.3933\n",
      "Observed mean for normal emails: 18.2145\n",
      "Estimated standard error: 2102.6761\n",
      "Wald statistic: 0.0410\n",
      "Estimated p-value: 0.9673076268688108\n",
      "\n",
      "Covariate 56\n",
      "observed mean for spam emails: 470.6194\n",
      "Observed mean for normal emails: 161.4709\n",
      "Estimated standard error: 16157.8063\n",
      "Wald statistic: 0.0191\n",
      "Estimated p-value: 0.9847349487782485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvals = []\n",
    "m = len(X_spam)\n",
    "n = len(X_norm)\n",
    "\n",
    "for i in range(N):\n",
    "    mu1 = X_spam[:, i].mean()\n",
    "    mu2 = X_norm[:, i].mean()\n",
    "    \n",
    "    s1 = X_spam[:, i].var()\n",
    "    s2 = X_norm[:, i].var()\n",
    "    se = np.sqrt((s1**2/m) + (s2**2/n))\n",
    "    \n",
    "    stat = mu1 - mu2\n",
    "    Wald = stat / se\n",
    "    \n",
    "    dist = norm()\n",
    "    pval = 2*dist.cdf(-np.abs(Wald))\n",
    "    \n",
    "    pvals.append(pval)\n",
    "    \n",
    "    print(f\"Covariate {i}\\nobserved mean for spam emails: {mu1:.4f}\\n\"\n",
    "          f\"Observed mean for normal emails: {mu2:.4f}\\n\"\n",
    "          f\"Estimated standard error: {se:.4f}\\n\"\n",
    "          f\"Wald statistic: {Wald:.4f}\\n\"\n",
    "          f\"Estimated p-value: {pval}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8ab452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indices ranked according to p-value,\n",
    "# from smallest p-value to largest\n",
    "indices = np.argsort(pvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0802489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 10 covariates with smallest p-value\n",
    "covars_to_keep = indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1da473fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reduced version of X with all covariates\n",
    "# dropped except for the 10 with smallest p-value\n",
    "X_red = X[:, covars_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3ec6",
   "metadata": {},
   "source": [
    "Now we compute the predictions for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a051126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = LDA(X_red,Y)\n",
    "Y_pred = np.array(list(map(h, X_red)))\n",
    "((m00, m01), (m10, m11)) = get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61e2ae98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2699,   89],\n",
       "       [ 802, 1011]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ea637",
   "metadata": {},
   "source": [
    "|&nbsp;|$\\hat h(x) = 0$| $\\hat h(x) = 1$|\n",
    "| :--- | :--- | :--- |\n",
    "|$Y=0$|2699|89|\n",
    "|$Y=1$|802|1011|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd8f490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA misclassification rate: 0.19365\n"
     ]
    }
   ],
   "source": [
    "mis = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "print(f'LDA misclassification rate: {mis:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5028edb",
   "metadata": {},
   "source": [
    "And finally we compute the predictions for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1decf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=None, fit_intercept=True,\n",
    "                         random_state=42, max_iter=10000)\n",
    "clf.fit(X_red, Y)\n",
    "Y_pred = clf.predict(X_red)\n",
    "((m00, m01), (m10, m11)) = get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f73d35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2669,  119],\n",
       "       [ 560, 1253]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_table(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b00717",
   "metadata": {},
   "source": [
    "|&nbsp;|$\\hat h(x) = 0$| $\\hat h(x) = 1$|\n",
    "| :--- | :--- | :--- |\n",
    "|$Y=0$|2669|119|\n",
    "|$Y=1$|560|1253|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dee80449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression misclassification rate: 0.14758\n"
     ]
    }
   ],
   "source": [
    "mis = (m01 + m10) / (m00 + m01 + m10 + m11)\n",
    "print(f'Logistic regression misclassification rate: {mis:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
